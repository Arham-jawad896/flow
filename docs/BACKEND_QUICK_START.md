# Flow ML Backend - Quick Start Guide

## ğŸš€ Your Backend is Ready!

Your production-ready Flow ML Backend is now running successfully! Here's what you have:

### âœ… What's Included

**Core Features:**
- âœ… JWT Authentication (register/login)
- âœ… File Upload (CSV, Excel, Images, Text)
- âœ… Data Preprocessing Engine
- âœ… Free/Premium User Tiers
- âœ… API Key Management
- âœ… Comprehensive Error Handling
- âœ… Security Logging
- âœ… Rate Limiting
- âœ… GroqCloud LLM Integration (optional)

**Free Tier Features:**
- 3 datasets per month
- Max 50MB file size
- Max 50,000 rows for CSV
- Basic preprocessing
- 3 API calls per month

**Premium Tier Features:**
- Unlimited datasets & file sizes
- Advanced preprocessing options
- Data augmentation
- Feature engineering
- Outlier removal
- Unlimited API calls
- LLM-powered insights

### ğŸŒ Access Your API

**Server Status:** âœ… Running on http://localhost:8000

**API Documentation:**
- **Swagger UI:** http://localhost:8000/docs
- **ReDoc:** http://localhost:8000/redoc
- **Health Check:** http://localhost:8000/health

### ğŸ”§ Quick Setup

1. **Start the server:**
   ```bash
   cd /home/arham/Desktop/Machine\ Learning/Projects/Flow/backend
   source env/bin/activate
   python start.py
   ```

2. **Test the setup:**
   ```bash
   python test_setup.py
   ```

3. **Optional - Add Groq API key for LLM features:**
   - Get API key from https://console.groq.com/
   - Add to `.env` file: `GROQ_API_KEY=your-key-here`

### ğŸ“‹ Key API Endpoints

**Authentication:**
- `POST /auth/register` - Register new user
- `POST /auth/login` - Login user
- `GET /auth/me` - Get current user info

**Dataset Management:**
- `POST /datasets/upload` - Upload dataset
- `GET /datasets` - List user datasets
- `GET /datasets/{id}` - Get dataset details
- `DELETE /datasets/{id}` - Delete dataset

**Preprocessing:**
- `POST /datasets/{id}/preprocess` - Start preprocessing
- `GET /datasets/{id}/preprocessing-status` - Check status
- `GET /datasets/{id}/download` - Download processed data

**LLM Features (if Groq API key is set):**
- `POST /datasets/{id}/analyze` - Analyze dataset with LLM
- `POST /datasets/{id}/recommend-preprocessing` - Get LLM recommendations
- `POST /datasets/{id}/explain-preprocessing` - Explain preprocessing steps

**User Management:**
- `GET /user/usage` - Get usage statistics
- `POST /user/upgrade` - Upgrade to premium
- `POST /api-keys` - Create API key
- `GET /api-keys` - List API keys

### ğŸ§ª Test Your API

**1. Register a user:**
```bash
curl -X POST "http://localhost:8000/auth/register" \
  -H "Content-Type: application/json" \
  -d '{
    "email": "test@example.com",
    "password": "password123",
    "full_name": "Test User"
  }'
```

**2. Login:**
```bash
curl -X POST "http://localhost:8000/auth/login" \
  -H "Content-Type: application/json" \
  -d '{
    "email": "test@example.com",
    "password": "password123"
  }'
```

**3. Upload a dataset (replace TOKEN with your access token):**
```bash
curl -X POST "http://localhost:8000/datasets/upload" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -F "file=@your_dataset.csv" \
  -F "dataset_name=My Test Dataset"
```

### ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ main.py                 # FastAPI application
â”œâ”€â”€ config.py              # Configuration
â”œâ”€â”€ database.py            # Database models
â”œâ”€â”€ auth.py                # Authentication
â”œâ”€â”€ schemas.py             # Pydantic models
â”œâ”€â”€ preprocessing.py       # Data preprocessing
â”œâ”€â”€ dataset_manager.py     # Dataset management
â”œâ”€â”€ user_tiers.py          # User tier logic
â”œâ”€â”€ groq_integration.py    # LLM integration
â”œâ”€â”€ error_handlers.py      # Error handling
â”œâ”€â”€ start.py               # Startup script
â”œâ”€â”€ test_setup.py          # Setup test
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ README.md              # Full documentation
â”œâ”€â”€ QUICK_START.md         # This file
â”œâ”€â”€ data/                  # Uploaded files
â”œâ”€â”€ logs/                  # Log files
â””â”€â”€ flow.db               # SQLite database
```

### ğŸ”’ Security Features

- JWT token authentication
- Password hashing with bcrypt
- Rate limiting (100 requests/minute)
- Input validation
- File type validation
- Security event logging
- CORS protection

### ğŸ“Š Monitoring

**Log Files:**
- `logs/flow_backend.log` - General logs
- `logs/errors.log` - Error logs
- `logs/security.log` - Security events
- `logs/performance.log` - Performance metrics

### ğŸš€ Production Deployment

For production deployment:

1. **Set environment variables:**
   ```bash
   export SECRET_KEY="your-production-secret-key"
   export DEBUG=False
   export GROQ_API_KEY="your-groq-key"
   ```

2. **Use a production database:**
   - PostgreSQL recommended
   - Update `DATABASE_URL` in config

3. **Use cloud storage:**
   - AWS S3, GCP Cloud Storage
   - Update file storage configuration

4. **Set up monitoring:**
   - Sentry for error tracking
   - DataDog/New Relic for performance
   - Custom webhook alerts

### ğŸ¯ Next Steps

1. **Frontend Integration:** Use the API endpoints to build your React/Next.js frontend
2. **Add Groq API Key:** Enable LLM features for intelligent data analysis
3. **Customize Preprocessing:** Modify preprocessing options based on your needs
4. **Scale Up:** Deploy to cloud with proper database and storage
5. **Monitor:** Set up production monitoring and alerting

### ğŸ“ Support

- **API Documentation:** http://localhost:8000/docs
- **Full Documentation:** README.md
- **Test Setup:** python test_setup.py

---

**ğŸ‰ Congratulations! Your Flow ML Backend is production-ready and running successfully!**
